# 传统机器学习模型

### 机器学习

- 特征提取
- 选择核函数来计算相似性
- 凸优化问题
- 漂亮的定理

### 几何学

- 抽取特征
- 描述几何（例如多相机）
- （非）凸优化
- 漂亮定理
- 如果假设满足了，效果很好

### 特征工程

- 特征工程是关键
- 特征描述子：SIFT，SURF
- 视觉词袋（聚类）
- 最后用SVM

### 历史发展

![image-20211125090031326](typora-user-images\\image-20211125090031326.png)



# 感知机

### 数学公式

> · 给定输入 X，权重 W 和偏移 b ，感知机输出：

$$
o = \sigma(<w,x> + b) \quad\quad \sigma(x)=\begin{cases}
1 \quad if \quad x > 0 \\
0 \quad otherwise \\ 
\end{cases}
$$



### 算法结构

![image-20211122161922064](typora-user-images\image-20211122161922064.png)

### 分类效果

![image-20211122162258279](typora-user-images\image-20211122162258279.png)

### 训练

> 使用批量为1的数据来进行训练，直到将所有数据进行准确预测。

### 缺点

> 无法解决XOR（异或）问题。

![image-20211122162502903](typora-user-images\image-20211122162502903.png)



# 多层感知机

### 数学公式

![image-20211122163553943](typora-user-images\image-20211122163553943.png)

### 激活函数

> 激活函数一定是非线性的，就可解决XOR问题。

> 若是采用线性激活函数，则多层感知机依旧是线性模型，并无法解决XOR问题。

![image-20211122163814034](typora-user-images\image-20211122163814034.png)

Sigmoid 激活函数：

> 取值区间：(0, 1)

![image-20211122164135640](typora-user-images\image-20211122164135640.png)

![img](https://pic4.zhimg.com/80/v2-8caff662e8fadb26406dd8b0ef326bcb_720w.jpg)

Tanh激活函数：

> 取值区间：(0, 1)

![image-20211122164402542](typora-user-images\image-20211122164402542.png)

![img](https://pic2.zhimg.com/80/v2-f0d760004e2e111f5608aa460d663f6d_720w.jpg)

ReLU激活函数：

![image-20211122164712641](typora-user-images\image-20211122164712641.png)

![img](https://pic2.zhimg.com/80/v2-e640a2cabd6586ed641ded24a813d371_720w.jpg)

### 多分类问题*

> ```html
> !!! 利用 softmax 函数将 y1 至 yk 的总和定为1， 同时模型也会习得类间的一些关系。
> ```

![image-20211122164949747](typora-user-images\image-20211122164949747.png)

数学公式：

![image-20211122165308789](typora-user-images\image-20211122165308789.png)

### 多隐藏层

![image-20211122165436595](typora-user-images\image-20211122165436595.png)

### 超参数

1. 隐藏层数。
2. 每层隐藏层的大小。



# 模型选择

### 训练误差

> 模型再训练数据上的误差。

### 泛化误差

> 模型在新数据上的误差。

### 验证数据集

> 一个用来评估模型好坏的数据集。

> 验证数据集一定不能跟训练数据集混在一起。

### 测试数据集

> 只用一次的数据集。

> 例如模型部署上线后接受的新数据。

### K-折交叉验证

> 在没有足够多数据时使用。数据量足够多，或深度神经网络一般不进行k-折交叉验证。

算法：

- 将训练数据分成k块
- For i = 1, ..., k
- 使用第 i 块作为验证数据集，其余的做作为训练集。
- End
- 报告 k 个验证集误差的平均
- 常用：k = 5 或 10



![å¨è¿éæå¥å¾çæè¿°](https://img-blog.csdnimg.cn/20190416195105246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzc0ODk3,size_16,color_FFFFFF,t_70)



### 过拟合和欠拟合

![image-20211122170713356](typora-user-images\image-20211122170713356.png)

#### 模型容量

- 拟合各种函数的能力
- 低容量的模型难以拟合训练数据
- 高容量的模型可以记住所有的训练数据

![image-20211122171447231](typora-user-images\image-20211122171447231.png)

#### 估计模型容量

- 参数的个数
- 参数值的选择范围



### 数据复杂度

- 样本个数
- 每个样本的元素个数
- 时间、空间结构
- 多样性（多分类）



# 权重衰退*

### 权重衰退*

> 最常见的用来处理过拟合的方法。
>
> 权重衰退通过正则项使模型参数不会过大，从而控制模型复杂度
>
> 正则项权重是控制模型复杂度的超参数

![image-20211122172745593](typora-user-images\image-20211122172745593.png)

- 通过限制参数值的选择范围来控制模型容量
- 通常不限制偏移b
- 小的 θ 意味着更强的正则项

> 使用均方范数作为柔性限制

- 对于每个 θ，都可以找到一个 λ 使上面的目标函数等价于下面

    ![image-20211122173647048](typora-user-images\image-20211122173647048.png)

- 超参数 λ 控制了正则项的重要程度

    - λ = 0 ：无作用
    - λ - > ∞, w* - > 0

> 因为模型复杂度等原因，会使模型过于拟合训练数据，导致对测试数据的预测效果开始变差。
>
> 因此，可以选择牺牲一点对训练数据拟合的准确率，来提高模型的泛化性（测试数据的准确率），即更平滑一些。

### 参数更新法则

- 计算梯度

    ![image-20211122180655520](typora-user-images\image-20211122180655520.png)

- 时间 t 更新参数

    ![image-20211122180645151](typora-user-images\image-20211122180645151.png)

    - 通常 ηλ < 1, 在深度学习中通常叫做权重衰退



### 丢弃法

> 一个好的模型需要对输入数据的扰动鲁棒（对有噪音的数据能表现出强健性）。
>
> 丢弃的概率是控制模型复杂度的超参数。

- 使用有噪音的数据 - Tikhonov正则
- 丢弃法：在层之间加入噪音

> 无偏差的加入噪音

- 对 x 加入噪音得到 x`，我们希望期望不变

![image-20211122181324936](typora-user-images\image-20211122181324936.png)

- 丢弃法对每个元素进行如下扰动

![image-20211122181335061](typora-user-images\image-20211122181335061.png)

- 根据求期望的公式

![image-20211122181458158](typora-user-images\image-20211122181458158.png)

# 数值稳定性*

### 梯度计算

- 考虑如下有 d 层的神经网络 ( y 不是预测值 )

    ![image-20211122182244443](typora-user-images\image-20211122182244443.png)

- 计算损失 l 关于参数 Wt 的梯度

    ![image-20211122182223898](typora-user-images\image-20211122182223898.png)

- 加入如下MLP （为了简单省略了偏移）

![image-20211122182642577](typora-user-images\image-20211122182642577.png)

### 梯度爆炸

- 使用 ReLU 作为激活函数

    ![image-20211122182824390](typora-user-images\image-20211122182824390.png)

![image-20211122182816302](typora-user-images\image-20211122182816302.png)

- 如果 d - t 很大，值将会很大



### 梯度爆炸的问题

- 值超出值域（inf）
- 对学习率敏感
    - 如果学习率太大 - > 大参数值 - > 更大的梯度
    - 如果学习率太小 - > 训练无进展
    - 我们可能需要在训练过程中不断调整学习率

### 梯度消失

- 使用 sigmoid 作为激活函数容易引起梯度消失

### 梯度消失的问题

- 梯度值变成 0
    - 对 16 位浮点数尤为严重
- 训练没有进展
    - 不管如何选择学习率
- 对于底部层尤为严重
    - 仅仅顶部层训练的较好
    - 无法让神经网络更深

### 让训练更加稳定*

- 让梯度值在合理的范围内
- 将乘法变成加法
    - ResNet，LSTM
- 归一化
    - 梯度归一化，梯度裁剪
- 合理的权重初始和激活函数

#### 让每层的方差是一个常数

- 将每层的输出和梯度都看作随机变量
- 让他们的均值（0，0）和方差（a，b）都保持一致

![image-20211122184229117](typora-user-images\image-20211122184229117.png)

#### 权重初始化

![image-20211122184826382](typora-user-images\image-20211122184826382.png)

均值：

![image-20211122184906297](typora-user-images\image-20211122184906297.png)

正向方差：

![image-20211122184959040](typora-user-images\image-20211122184959040.png)

得出下一层输入的方差为上一层输出的![image-20211122185137685](typora-user-images\image-20211122185137685.png)倍，若要保持数值稳定，![image-20211122185137685](typora-user-images\image-20211122185137685.png)需要等于 1。

反向方差：

![image-20211122185231879](typora-user-images\image-20211122185231879.png)

得出上一层输出的方差为下一层输入的![image-20211122185541806](typora-user-images\image-20211122185541806.png)倍，若要保持数值稳定，![image-20211122185548757](typora-user-images\image-20211122185548757.png)需要等于 1。

#### Xavier 初始

- 难以同时满足![image-20211122185713496](typora-user-images\image-20211122185713496.png)
- Xavier 使得 <img src="typora-user-images\image-20211122185909182.png" alt="image-20211122185909182" style="zoom:67%;" />
    - 正态分布![image-20211122185940026](typora-user-images\image-20211122185940026.png)
    - 均匀分布![image-20211122185956997](typora-user-images\image-20211122185956997.png)
        - 分布![image-20211122190350405](typora-user-images\image-20211122190350405.png)和方差![image-20211122190416469](typora-user-images\image-20211122190416469.png)

- 适配权重形态变换，特别是![image-20211122190442188](typora-user-images\image-20211122190442188.png)

### 激活函数的选择

- 假设![image-20211122190607955](typora-user-images\image-20211122190607955.png)

![image-20211122190644683](typora-user-images\image-20211122190644683.png)

> 因此，若是 α = 1，会使数值的传递更具有稳定性。
>
> 因为在 [0, 1] 区间内sigmoid函数的导数并不趋近于1，所以当用 sigmoid 函数作为激活函数时，会容易产生梯度消失的问题。
>
> 相较于 sigmoid 函数，4 * sigmoid(x) - 2 可能会更好一点

